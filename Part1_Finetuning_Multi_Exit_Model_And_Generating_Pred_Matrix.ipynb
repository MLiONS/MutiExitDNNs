{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part-1 : Section A\n",
        "Training a multi-exit ElasticBERT model on SST-2 dataset"
      ],
      "metadata": {
        "id": "qZrkmpkW8Wbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CkfzdLG9C1T"
      },
      "outputs": [],
      "source": [
        "# The code closely follows the original ElasticBERT repository\n",
        "# Feature to train models with a given exit configuration is added\n",
        "!git clone https://github.com/MLiONS/MutiExitDNNs.git\n",
        "\n",
        "%cd /content/MutiExitDNNs/ElasticBERT\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#All the hyper-parameters/ location to training dataset are set in\n",
        "#MultiExitDNNs -> finetune-dynamic -> finetune_elue_entropy.sh file\n",
        "\n",
        "#1)Set the correct location to SST-2 dataset\n",
        "#All models are trained on SST-2 \"train\" split and evaluated on \"dev\" split\n",
        "#\"train.tsv\" and \"dev.tsv\" are expected to be in ELUE_DIR/TASK_NAME\n",
        "#You can set both ELUE_DIR and TASK_NAME in finetune_elue_entropy.sh\n",
        "#Or change the dataset directory using \"data_dir\" option\n",
        "\n",
        "#2)Please change the \"num_output_layers\" option as per the desired exit-configuration\n",
        "\n",
        "#3)Model checkpoints will be saved at \"output_dir\" and\n",
        "#logs will be available at \"log_dir\"\n",
        "\n",
        "!bash /content/MutiExitDNNs/ElasticBERT/finetune-dynamic/finetune_elue_entropy.sh"
      ],
      "metadata": {
        "id": "A26ZgAoxPyiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QS9n0Bbn_nZj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-1 : Section B\n",
        "Generating the prediction matrix"
      ],
      "metadata": {
        "id": "3tfcWxU28kkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "acoXLlG2y1ty"
      },
      "outputs": [],
      "source": [
        "#Evaluation on other datasets-IMDb or Yelp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb9mBUI5y1q-"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer as ElasticBertTokenizer\n",
        "\n",
        "#Set the current directory location inside \"finetune-dynamic\" folder\n",
        "%cd /content/MutiExitDNNs/ElasticBERT/finetune-dynamic\n",
        "\n",
        "from models.configuration_elasticbert import ElasticBertConfig\n",
        "from models.modeling_elasticbert_entropy import ElasticBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZkT2IrL2b90d"
      },
      "outputs": [],
      "source": [
        "#Set location to the best performing model\n",
        "#Model checkpoints are saved at \"output_dir\" from Part-1: Section A\n",
        "\n",
        "checkpoint = '/content/MutiExitDNNs/ElasticBERT/ckpts/elue/entropy/SST-2/checkpoint-50'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cCv-3N0CaM0s"
      },
      "outputs": [],
      "source": [
        "config = ElasticBertConfig.from_pretrained(checkpoint)\n",
        "tokenizer = ElasticBertTokenizer.from_pretrained(checkpoint)\n",
        "model = ElasticBertForSequenceClassification.from_pretrained(checkpoint)\n",
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AFt7llc_g130"
      },
      "outputs": [],
      "source": [
        "def get_args(arg_vec):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--num_hidden_layers\",\n",
        "        default=None,\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help='The number of layers to import.',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_output_layers\",\n",
        "        nargs = 12,\n",
        "        default=None,\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help='The number of layers to output.',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to pre-trained model or shortcut name.\",\n",
        "    )    \n",
        "    parser.add_argument(\n",
        "        \"--task_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The name of the task to train selected in the list.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the logs will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--spec_eval\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=False,\n",
        "        help=\"'Set as train or test based on specific split on which to evaluate'\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--patience\",\n",
        "        default='0',\n",
        "        type=str,\n",
        "        required=False,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--regression_threshold\",\n",
        "        default=0,\n",
        "        type=float,\n",
        "        required=False,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--early_exit_entropy\",\n",
        "        default='0.1',\n",
        "        type=str,\n",
        "        required=False,\n",
        "    )\n",
        "    # Other parameters  \n",
        "    parser.add_argument(\n",
        "        \"--load\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The path of ckpts used to continue training.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--config_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained config name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "             \"than this will be truncated, sequences shorter will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Whether to use debug mode.\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Run evaluation during training at each logging step.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Set this flag if you are using an uncased model.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_train_batch_size\",\n",
        "        default=8,\n",
        "        type=int,\n",
        "        help=\"Batch size per GPU/CPU for training.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\",\n",
        "        default=1,\n",
        "        type=int,\n",
        "        help=\"Batch size per GPU/CPU for evaluation.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        default=5e-5,\n",
        "        type=float,\n",
        "        help=\"The initial learning rate for Adam.\",\n",
        "    )\n",
        "    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\",\n",
        "        default=3.0,\n",
        "        type=float,\n",
        "        help=\"Total number of training epochs to perform.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument(\"--warmup_rate\", default=0, type=float, help=\"Linear warmup over warmup_rate.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--save_steps\",\n",
        "        type=int,\n",
        "        default=500,\n",
        "        help=\"Save checkpoint every X updates steps.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Overwrite the content of the output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Overwrite the cached training and evaluation sets\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--not_save_model\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Do not save model checkpoints\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=6, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "             \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--local_rank\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"For distributed training: local_rank\",\n",
        "    )\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args(arg_vec)\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E2cX_ym5pJwl"
      },
      "outputs": [],
      "source": [
        "from evaluations import evaluate_elue_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jCJmL5-yez21"
      },
      "outputs": [],
      "source": [
        "  ELUE_DIR='/content/drive/MyDrive/elue_data'\n",
        "  TASK_NAME='SST-2'\n",
        "  \n",
        "  arg_vec= ['--model_name_or_path', 'fnlp/elasticbert-base', \n",
        "  '--task_name', 'SST-2', \\\n",
        "  '--do_train', \\\n",
        "  '--do_lower_case', \\\n",
        "  '--data_dir', \"/content/drive/MyDrive/elue_data/SST-2\", \\\n",
        "  '--log_dir', '/content/ElasticBERT/logs/elue/entropy/SST-2TestCheck', \\\n",
        "  '--output_dir', '/content/ElasticBERT/ckpts/elue/entropy/SST-2TestCheck', \\\n",
        "  '--num_hidden_layers', '12', \\\n",
        "  '--num_output_layers', '1', '1', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', \\\n",
        "  '--max_seq_length', '128', \\\n",
        "  '--per_gpu_train_batch_size', '32', \\\n",
        "  '--per_gpu_eval_batch_size',' 32', \\\n",
        "  '--learning_rate', '2e-5', \\\n",
        "  '--weight_decay', '0.1', \\\n",
        "  '--save_steps', '50', \\\n",
        "  '--logging_steps', '50', \\\n",
        "  '--num_train_epochs', '5',  \\\n",
        "  '--warmup_rate', '0.06', \\\n",
        "  '--evaluate_during_training', \\\n",
        "  '--overwrite_output_dir' \n",
        "]\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "args = get_args(arg_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLZpjqakoP9G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend=\"nccl\")\n",
        "    args.n_gpu = 1\n",
        "args.device = device\n",
        "\n",
        "args.output_mode = 'classification'\n",
        "\n",
        "print(args.device)\n",
        "model.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Selection\n",
        "dataset = 'IMDb' #or 'Yelp'\n",
        "\n",
        "#To check model performance on SST-2 dev split:\n",
        "#Please set dataset = 'SST-2' and data_split='dev'"
      ],
      "metadata": {
        "id": "rNSuHnH-mSPt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_preds(eval_dataset='IMDb', data_split='train'):\n",
        "  args.spec_eval=data_split\n",
        "  args.task_name=eval_dataset.lower()\n",
        "  args.data_dir=ELUE_DIR + '/'+args.task_name\n",
        "\n",
        "  results_all, exit_preds, op_labels = evaluate_elue_entropy(args, model, tokenizer)\n",
        "\n",
        "  exit_preds_list = np.stack(exit_preds, axis=1)\n",
        "  df = pd.DataFrame((exit_preds_list) )\n",
        "  df['op_labels'] = op_labels\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "ZyrFjs52hwRW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_preds(eval_dataset=dataset, data_split='train')\n",
        "df_test = get_preds(eval_dataset=dataset, data_split='test')\n",
        "\n",
        "df_tot = pd.concat([df_train, df_test])\n",
        "df_tot = df_tot.reset_index(drop=True)\n",
        "print(df_tot.head())\n",
        "\n",
        "df_tot.to_csv(r'/content/Exit_Predictions_TrainTest_IMDb_8exits.csv',sep ='\\t', index = False)"
      ],
      "metadata": {
        "id": "VOpSU7ShmMU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AlgCgxMQm17O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Part1_Finetuning_Multi_Exit_Model_And_Generating_Pred_Matrix.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qZrkmpkW8Wbj",
        "3tfcWxU28kkl"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}